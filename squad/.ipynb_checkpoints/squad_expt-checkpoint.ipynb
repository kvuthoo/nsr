{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lightweight-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import zipfile\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "from utils import tokenizer, clean_text, word_tokenize, build_vocab, build_embeddings, convert_idx\n",
    "import layers\n",
    "cuda = False\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "missing-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment ID\n",
    "exp = \"exp-1\"\n",
    "\n",
    "\n",
    "# URL to download SQuAD dataset 2.0\n",
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset\"\n",
    "\n",
    "# data directories\n",
    "data_dir = \"/home/cheeta/nsr/squad/Data/squad/\"\n",
    "train_dir = data_dir + \"train/\"\n",
    "dev_dir = data_dir + \"dev/\"\n",
    "\n",
    "# model paths\n",
    "spacy_en = \"/home/cheeta/nsr/squad/Data/spacy/en_core_web_sm-2.0.0/en_core_web_sm/en_core_web_sm-2.0.0\"\n",
    "glove = \"/home/cheeta/nsr/squad/Data/glove.6B/\" + \"glove.6B.{}d.txt\"\n",
    "squad_models = \"output/\" + exp\n",
    "\n",
    "# preprocessing values\n",
    "max_words = -1\n",
    "word_embedding_size = 100\n",
    "char_embedding_size = 8\n",
    "max_len_context = 400\n",
    "max_len_question = 50\n",
    "max_len_word = 25\n",
    "\n",
    "# training hyper-parameters\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "drop_prob = 0.2\n",
    "hidden_size = 100\n",
    "char_channel_width = 5\n",
    "char_channel_size = 100\n",
    "cuda = False\n",
    "\n",
    "# Utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "expanded-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE DATESET\n",
    "\n",
    "def download_squad(url, filename, out_dir):\n",
    "    # path for local file.\n",
    "    save_path = os.path.join(out_dir, filename)\n",
    "\n",
    "    # check if the file already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        # check if the output director exists, otherwise create it.\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        print(\"Downloading\", filename, \"...\")\n",
    "\n",
    "        # download the dataset\n",
    "        url = os.path.join(url, filename)\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url, filename=save_path)\n",
    "\n",
    "    print(\"File downloaded successfully!\")\n",
    "\n",
    "    if filename.endswith(\".zip\"):\n",
    "        # unpack the zip-file.\n",
    "        print(\"Extracting ZIP file...\")\n",
    "        zipfile.ZipFile(file=filename, mode=\"r\").extractall(out_dir)\n",
    "        print(\"File extracted successfully!\")\n",
    "    elif filename.endswith((\".tar.gz\", \".tgz\")):\n",
    "        # unpack the tar-ball.\n",
    "        print(\"Extracting TAR file...\")\n",
    "        tarfile.open(name=filename, mode=\"r:gz\").extractall(out_dir)\n",
    "        print(\"File extracted successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afraid-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQUAD PRE PROCESSING\n",
    "\n",
    "\n",
    "class SquadPreprocessor:\n",
    "    def __init__(self, data_dir, train_filename, dev_filename, tokenizer):\n",
    "        self.data_dir = data_dir\n",
    "        self.train_filename = train_filename\n",
    "        self.dev_filename = dev_filename\n",
    "        self.data = None\n",
    "        self.tokenizer = utils.tokenizer\n",
    "\n",
    "    def load_data(self, filename=\"train-v2.0.json\"):\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        with open(filepath) as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def split_data(self, filename):\n",
    "        self.load_data(filename)\n",
    "        sub_dir = filename.split('-')[0]\n",
    "\n",
    "        # create a subdirectory for Train and Dev data\n",
    "        if not os.path.exists(os.path.join(self.data_dir, sub_dir)):\n",
    "            os.makedirs(os.path.join(self.data_dir, sub_dir))\n",
    "\n",
    "        with open(os.path.join(self.data_dir, sub_dir, sub_dir + '.context'), 'w', encoding=\"utf-8\") as context_file,\\\n",
    "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.question'), 'w', encoding=\"utf-8\") as question_file,\\\n",
    "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.answer'), 'w', encoding=\"utf-8\") as answer_file,\\\n",
    "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.labels'), 'w', encoding=\"utf-8\") as labels_file:\n",
    "\n",
    "            # loop over the data\n",
    "            for article_id in tqdm.tqdm(range(len(self.data['data']))):\n",
    "                list_paragraphs = self.data['data'][article_id]['paragraphs']\n",
    "                # loop over the paragraphs\n",
    "                for paragraph in list_paragraphs:\n",
    "                    context = paragraph['context']\n",
    "                    context = clean_text(context)\n",
    "                    context_tokens = [w for w in word_tokenize(context) if w]\n",
    "                    spans = convert_idx(context, context_tokens)\n",
    "                    qas = paragraph['qas']\n",
    "                    # loop over Q/A\n",
    "                    for qa in qas:\n",
    "                        question = qa['question']\n",
    "                        question = clean_text(question)\n",
    "                        question_tokens = [w for w in word_tokenize(question) if w]\n",
    "                        if sub_dir == \"train\":\n",
    "                            # select only one ground truth, the top answer, if any answer\n",
    "                            answer_ids = 1 if qa['answers'] else 0\n",
    "                        else:\n",
    "                            answer_ids = len(qa['answers'])\n",
    "                        labels = []\n",
    "                        if answer_ids:\n",
    "                            for answer_id in range(answer_ids):\n",
    "                                answer = qa['answers'][answer_id]['text']\n",
    "                                answer = clean_text(answer)\n",
    "                                answer_tokens = [w for w in word_tokenize(answer) if w]\n",
    "                                answer_start = qa['answers'][answer_id]['answer_start']\n",
    "                                answer_stop = answer_start + len(answer)\n",
    "                                answer_span = []\n",
    "                                for idx, span in enumerate(spans):\n",
    "                                    if not (answer_stop <= span[0] or answer_start >= span[1]):\n",
    "                                        answer_span.append(idx)\n",
    "                                if not answer_span:\n",
    "                                    continue\n",
    "                                labels.append(str(answer_span[0]) + ' ' + str(answer_span[-1]))\n",
    "\n",
    "                            # write to file\n",
    "                            context_file.write(' '.join([token for token in context_tokens]) + '\\n')\n",
    "                            question_file.write(' '.join([token for token in question_tokens]) + '\\n')\n",
    "                            answer_file.write(' '.join([token for token in answer_tokens]) + '\\n')\n",
    "                            labels_file.write(\"|\".join(labels) + \"\\n\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.split_data(train_filename)\n",
    "        self.split_data(dev_filename)\n",
    "\n",
    "    def extract_features(self, max_len_context= max_len_context, max_len_question= max_len_question,\n",
    "                         max_len_word= max_len_word, is_train=True):\n",
    "        # choose the right directory\n",
    "        directory = \"train\" if is_train else \"dev\"\n",
    "\n",
    "        # load context\n",
    "        with open(os.path.join(self.data_dir, directory, directory + \".context\"), \"r\", encoding=\"utf-8\") as c:\n",
    "            context = c.readlines()\n",
    "        # load questions\n",
    "        with open(os.path.join(self.data_dir, directory, directory + \".question\"), \"r\", encoding=\"utf-8\") as q:\n",
    "            question = q.readlines()\n",
    "        # load answer\n",
    "        with open(os.path.join(self.data_dir, directory, directory + \".labels\"), \"r\", encoding=\"utf-8\") as l:\n",
    "            labels = l.readlines()\n",
    "\n",
    "        # clean and tokenize context and question\n",
    "        context = [[w for w in word_tokenize(clean_text(doc.strip('\\n')))] for doc in context]\n",
    "        question = [[w for w in word_tokenize(clean_text(doc.strip('\\n')))] for doc in question]\n",
    "\n",
    "        # download vocabulary if not done yet\n",
    "        if directory == \"train\":\n",
    "            labels = [np.array(l.strip(\"\\n\").split(), dtype=np.int32) for l in labels]\n",
    "\n",
    "            word_vocab, word2idx, char_vocab, char2idx = build_vocab(directory + \".context\", directory + \".question\",\n",
    "                                                                     \"word_vocab.pkl\", \"word2idx.pkl\", \"char_vocab.pkl\",\n",
    "                                                                     \"char2idx.pkl\", is_train, max_words)\n",
    "            # create an embedding matrix from the vocabulary with pretrained vectors (GloVe) for words\n",
    "            build_embeddings(word_vocab, embedding_path= glove, output_path=\"word_embeddings.pkl\",\n",
    "                             vec_size= word_embedding_size)\n",
    "            build_embeddings(char_vocab, embedding_path=\"\", output_path=\"char_embeddings.pkl\",\n",
    "                             vec_size= char_embedding_size)\n",
    "\n",
    "        else:\n",
    "            labels = np.array([l.strip(\"\\n\") for l in labels])\n",
    "\n",
    "            with open(os.path.join(self.data_dir, \"train\", \"word2idx.pkl\"), \"rb\") as wi,\\\n",
    "                 open(os.path.join(self.data_dir, \"train\", \"char2idx.pkl\"), \"rb\") as ci:\n",
    "                    word2idx = pickle.load(wi)\n",
    "                    char2idx = pickle.load(ci)\n",
    "\n",
    "        print(\"Number of questions before filtering:\", len(question))\n",
    "        filter = [len(c) < max_len_context and max([len(w) for w in c]) < max_len_word and\n",
    "                  len(q) < max_len_question and max([len(w) for w in q]) < max_len_word and\n",
    "                  len(q) > 3 for c, q in zip(context, question)]\n",
    "        context, question, labels = zip(*[(c, q, l) for c, q, l, f in zip(\n",
    "                                          context, question, labels, filter) if f])\n",
    "        print(\"Number of questions after filtering \", len(question))\n",
    "\n",
    "        # replace the tokenized words with their associated ID in the vocabulary\n",
    "        context_idxs = []\n",
    "        context_char_idxs = []\n",
    "        question_idxs = []\n",
    "        question_char_idxs = []\n",
    "        for i, (c, q) in tqdm.tqdm(enumerate(zip(context, question))):\n",
    "            # create empty numpy arrays\n",
    "            context_idx = np.zeros([max_len_context], dtype=np.int32)\n",
    "            question_idx = np.zeros([max_len_question], dtype=np.int32)\n",
    "            context_char_idx = np.zeros([max_len_context, max_len_word], dtype=np.int32)\n",
    "            question_char_idx = np.zeros([max_len_question, max_len_word], dtype=np.int32)\n",
    "\n",
    "            # replace 0 values with word and char IDs\n",
    "            for j, word in enumerate(c):\n",
    "                if word in word2idx:\n",
    "                    context_idx[j] = word2idx[word]\n",
    "                else:\n",
    "                    context_idx[j] = 1\n",
    "                for k, char in enumerate(word):\n",
    "                    if char in char2idx:\n",
    "                        context_char_idx[j, k] = char2idx[char]\n",
    "                    else:\n",
    "                        context_char_idx[j, k] = 1\n",
    "            context_idxs.append(context_idx)\n",
    "            context_char_idxs.append(context_char_idx)\n",
    "\n",
    "            for j, word in enumerate(q):\n",
    "                if word in word2idx:\n",
    "                    question_idx[j] = word2idx[word]\n",
    "                else:\n",
    "                    question_idx[j] = 1\n",
    "                for k, char in enumerate(word):\n",
    "                    if char in char2idx:\n",
    "                        question_char_idx[j, k] = char2idx[char]\n",
    "                    else:\n",
    "                        question_char_idx[j, k] = 1\n",
    "            question_idxs.append(question_idx)\n",
    "            question_char_idxs.append(question_char_idx)\n",
    "        # save features as numpy arrays\n",
    "        np.savez(os.path.join(self.data_dir, directory, directory + \"_features\"),\n",
    "                 context_idxs=np.array(context_idxs),\n",
    "                 context_char_idxs=np.array(context_char_idxs),\n",
    "                 question_idxs=np.array(question_idxs),\n",
    "                 question_char_idxs=np.array(question_char_idxs),\n",
    "                 label=np.array(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ...\n",
    "\n",
    "train_filename = \"train-v2.0.json\"\n",
    "dev_filename = \"dev-v2.0.json\"\n",
    "\n",
    "download_squad(url, train_filename, data_dir)\n",
    "download_squad(url, dev_filename, data_dir)\n",
    "\n",
    "p = SquadPreprocessor(data_dir, train_filename, dev_filename, utils.tokenizer)\n",
    "p.preprocess()\n",
    "\n",
    "p.extract_features(max_len_context, max_len_question,\n",
    "                   max_len_word, is_train=True)\n",
    "p.extract_features(max_len_context, max_len_question,\n",
    "                   max_len_word, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader  -- pytorch\n",
    "\n",
    "\n",
    "class SquadDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset for SQuAD data compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, w_context, c_context, w_question, c_question, labels):\n",
    "        \"\"\"Set the path for context, question and labels.\"\"\"\n",
    "        self.w_context = w_context\n",
    "        self.c_context = c_context\n",
    "        self.w_question = w_question\n",
    "        self.c_question = c_question\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data tuple of the form ( word context, character context, word question,\n",
    "         character question, answer).\"\"\"\n",
    "        return self.w_context[index], self.c_context[index], self.w_question[index], self.c_question[index],\\\n",
    "               self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.w_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class BiDAF(nn.Module):\n",
    "    \"\"\"Baseline BiDAF model for SQuAD.\n",
    "    Based on the paper:\n",
    "    \"Bidirectional Attention Flow for Machine Comprehension\"\n",
    "    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "    (https://arxiv.org/abs/1611.01603).\n",
    "    Follows a high-level structure commonly found in SQuAD models:\n",
    "        - Embedding layer: Embed word indices to get word vectors.\n",
    "        - Encoder layer: Encode the embedded sequence.\n",
    "        - Attention layer: Apply an attention mechanism to the encoded sequence.\n",
    "        - Model encoder layer: Encode the sequence again.\n",
    "        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n",
    "    Args:\n",
    "        word_vectors (torch.Tensor): Pre-trained word vectors.\n",
    "        hidden_size (int): Number of features in the hidden state at each layer.\n",
    "        drop_prob (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_vectors, char_vectors, hidden_size, drop_prob=0.):\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.emb = layers.Embedding(word_vectors=word_vectors,\n",
    "                                    char_vectors=char_vectors,\n",
    "                                    hidden_size=hidden_size,\n",
    "                                    drop_prob=drop_prob)\n",
    "\n",
    "        self.enc = layers.RNNEncoder(input_size=hidden_size * 2,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=1,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.att = layers.BiDAFAttention(hidden_size=2 * hidden_size,\n",
    "                                         drop_prob=drop_prob)\n",
    "\n",
    "        self.mod = layers.RNNEncoder(input_size=8 * hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=2,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.out = layers.BiDAFOutput(hidden_size=hidden_size,\n",
    "                                      drop_prob=drop_prob)\n",
    "\n",
    "    def forward(self, cw_idxs, cc_idxs, qw_idxs, qc_idxs):\n",
    "        c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "        q_mask = torch.zeros_like(qw_idxs) != qw_idxs\n",
    "        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n",
    "\n",
    "        c_emb = self.emb(cw_idxs, cc_idxs)         # (batch_size, c_len, hidden_size)\n",
    "        q_emb = self.emb(qw_idxs, qc_idxs)         # (batch_size, q_len, hidden_size)\n",
    "\n",
    "        c_enc = self.enc(c_emb, c_len)    # (batch_size, c_len, 2 * hidden_size)\n",
    "        q_enc = self.enc(q_emb, q_len)    # (batch_size, q_len, 2 * hidden_size)\n",
    "\n",
    "        att = self.att(c_enc, q_enc,\n",
    "                       c_mask, q_mask)    # (batch_size, c_len, 8 * hidden_size)\n",
    "\n",
    "        mod = self.mod(att, c_len)        # (batch_size, c_len, 2 * hidden_size)\n",
    "\n",
    "        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "# preprocessing values used for training\n",
    "prepro_params = {\n",
    "    \"max_words\": max_words,\n",
    "    \"word_embedding_size\": word_embedding_size,\n",
    "    \"char_embedding_size\": char_embedding_size,\n",
    "    \"max_len_context\": max_len_context,\n",
    "    \"max_len_question\": max_len_question,\n",
    "    \"max_len_word\": max_len_word\n",
    "}\n",
    "\n",
    "# hyper-parameters setup\n",
    "hyper_params = {\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"char_channel_width\": char_channel_width,\n",
    "    \"char_channel_size\": char_channel_size,\n",
    "    \"drop_prob\": drop_prob,\n",
    "    \"cuda\": cuda,\n",
    "    \"pretrained\": pretrained\n",
    "}\n",
    "\n",
    "experiment_params = {\"preprocessing\": prepro_params, \"model\": hyper_params}\n",
    "\n",
    "# train on GPU if CUDA variable is set to True (a GPU with CUDA is needed to do so)\n",
    "device = torch.device(\"cuda\" if hyper_params[\"cuda\"] else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# define a path to save experiment logs\n",
    "experiment_path = \"output/{}\".format(exp)\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "# save the preprocesisng and model parameters used for this training experiemnt\n",
    "with open(os.path.join(experiment_path, \"config_{}.json\".format(exp)), \"w\") as f:\n",
    "    json.dump(experiment_params, f)\n",
    "\n",
    "# start TensorboardX writer\n",
    "writer = SummaryWriter(experiment_path)\n",
    "\n",
    "# open features file and store them in individual variables (train + dev)\n",
    "train_features = np.load(os.path.join(train_dir, \"train_features.npz\"))\n",
    "t_w_context, t_c_context, t_w_question, t_c_question, t_labels = train_features[\"context_idxs\"],\\\n",
    "                                                                 train_features[\"context_char_idxs\"],\\\n",
    "                                                                 train_features[\"question_idxs\"],\\\n",
    "                                                                 train_features[\"question_char_idxs\"],\\\n",
    "                                                                 train_features[\"label\"]\n",
    "\n",
    "dev_features = np.load(os.path.join(dev_dir, \"dev_features.npz\"))\n",
    "d_w_context, d_c_context, d_w_question, d_c_question, d_labels = dev_features[\"context_idxs\"],\\\n",
    "                                                                 dev_features[\"context_char_idxs\"],\\\n",
    "                                                                 dev_features[\"question_idxs\"],\\\n",
    "                                                                 dev_features[\"question_char_idxs\"],\\\n",
    "                                                                 dev_features[\"label\"]\n",
    "\n",
    "# load the embedding matrix created for our word vocabulary\n",
    "with open(os.path.join(train_dir, \"word_embeddings.pkl\"), \"rb\") as e:\n",
    "    word_embedding_matrix = pickle.load(e)\n",
    "with open(os.path.join(train_dir, \"char_embeddings.pkl\"), \"rb\") as e:\n",
    "    char_embedding_matrix = pickle.load(e)\n",
    "\n",
    "# load mapping between words and idxs\n",
    "with open(os.path.join(train_dir, \"word2idx.pkl\"), \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "idx2word = dict([(y, x) for x, y in word2idx.items()])\n",
    "\n",
    "# transform them into Tensors\n",
    "word_embedding_matrix = torch.from_numpy(np.array(word_embedding_matrix)).type(torch.float32)\n",
    "char_embedding_matrix = torch.from_numpy(np.array(char_embedding_matrix)).type(torch.float32)\n",
    "\n",
    "# load datasets\n",
    "train_dataset = SquadDataset(t_w_context, t_c_context, t_w_question, t_c_question, t_labels)\n",
    "valid_dataset = SquadDataset(d_w_context, d_c_context, d_w_question, d_c_question, d_labels)\n",
    "\n",
    "# load data generators\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=hyper_params[\"batch_size\"],\n",
    "                              num_workers=4)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=hyper_params[\"batch_size\"],\n",
    "                              num_workers=4)\n",
    "\n",
    "print(\"Length of training data loader is:\", len(train_dataloader))\n",
    "print(\"Length of valid data loader is:\", len(valid_dataloader))\n",
    "\n",
    "# load the model\n",
    "model = BiDAF(word_vectors=word_embedding_matrix,\n",
    "              char_vectors=char_embedding_matrix,\n",
    "              hidden_size=hyper_params[\"hidden_size\"],\n",
    "              drop_prob=hyper_params[\"drop_prob\"])\n",
    "if hyper_params[\"pretrained\"]:\n",
    "    model.load_state_dict(torch.load(os.path.join(experiment_path, \"model.pkl\"))[\"state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), hyper_params[\"learning_rate\"], weight_decay=1e-4)\n",
    "\n",
    "# best loss so far\n",
    "if hyper_params[\"pretrained\"]:\n",
    "    best_valid_loss = torch.load(os.path.join(experiment_path, \"model.pkl\"))[\"best_valid_loss\"]\n",
    "    epoch_checkpoint = torch.load(os.path.join(experiment_path, \"model_last_checkpoint.pkl\"))[\"epoch\"]\n",
    "    print(\"Best validation loss obtained after {} epochs is: {}\".format(epoch_checkpoint, best_valid_loss))\n",
    "else:\n",
    "    best_valid_loss = 100\n",
    "    epoch_checkpoint = 0\n",
    "\n",
    "# train the Model\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(hyper_params[\"num_epochs\"]):\n",
    "    print(\"##### epoch {:2d}\".format(epoch + 1))\n",
    "    model.train()\n",
    "    train_losses = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        w_context, c_context, w_question, c_question, label1, label2 = batch[0].long().to(device),\\\n",
    "                                                                       batch[1].long().to(device), \\\n",
    "                                                                       batch[2].long().to(device), \\\n",
    "                                                                       batch[3].long().to(device), \\\n",
    "                                                                       batch[4][:, 0].long().to(device),\\\n",
    "                                                                       batch[4][:, 1].long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred1, pred2 = model(w_context, c_context, w_question, c_question)\n",
    "        loss = criterion(pred1, label1) + criterion(pred2, label2)\n",
    "        train_losses += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    writer.add_scalars(\"train\", {\"loss\": np.round(train_losses / len(train_dataloader), 2),\n",
    "                                 \"epoch\": epoch + 1})\n",
    "    print(\"Train loss of the model at epoch {} is: {}\".format(epoch + 1, np.round(train_losses /\n",
    "                                                                                  len(train_dataloader), 2)))\n",
    "\n",
    "    model.eval()\n",
    "    valid_losses = 0\n",
    "    valid_em = 0\n",
    "    valid_f1 = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valid_dataloader):\n",
    "            w_context, c_context, w_question, c_question, labels = batch[0].long().to(device), \\\n",
    "                                                                   batch[1].long().to(device), \\\n",
    "                                                                   batch[2].long().to(device), \\\n",
    "                                                                   batch[3].long().to(device), \\\n",
    "                                                                   batch[4]\n",
    "\n",
    "            first_labels = torch.tensor([[int(a) for a in l.split(\"|\")[0].split(\" \")]\n",
    "                                         for l in labels], dtype=torch.int64).to(device)\n",
    "            pred1, pred2 = model(w_context, c_context, w_question, c_question)\n",
    "            loss = criterion(pred1, first_labels[:, 0]) + criterion(pred2, first_labels[:, 1])\n",
    "            valid_losses += loss.item()\n",
    "            em, f1 = compute_batch_metrics(w_context, idx2word, pred1, pred2, labels)\n",
    "            valid_em += em\n",
    "            valid_f1 += f1\n",
    "            n_samples += w_context.size(0)\n",
    "\n",
    "        writer.add_scalars(\"valid\", {\"loss\": np.round(valid_losses / len(valid_dataloader), 2),\n",
    "                                     \"EM\": np.round(valid_em / n_samples, 2),\n",
    "                                     \"F1\": np.round(valid_f1 / n_samples, 2),\n",
    "                                     \"epoch\": epoch + 1})\n",
    "        print(\"Valid loss of the model at epoch {} is: {}\".format(epoch + 1, np.round(valid_losses /\n",
    "                                                                                      len(valid_dataloader), 2)))\n",
    "        print(\"Valid EM of the model at epoch {} is: {}\".format(epoch + 1, np.round(valid_em / n_samples, 2)))\n",
    "        print(\"Valid F1 of the model at epoch {} is: {}\".format(epoch + 1, np.round(valid_f1 / n_samples, 2)))\n",
    "\n",
    "    # save last model weights\n",
    "    save_checkpoint({\n",
    "        \"epoch\": epoch + 1 + epoch_checkpoint,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best_valid_loss\": np.round(valid_losses / len(valid_dataloader), 2)\n",
    "    }, True, os.path.join(experiment_path, \"model_last_checkpoint.pkl\"))\n",
    "\n",
    "    # save model with best validation error\n",
    "    is_best = bool(np.round(valid_losses / len(valid_dataloader), 2) < best_valid_loss)\n",
    "    best_valid_loss = min(np.round(valid_losses / len(valid_dataloader), 2), best_valid_loss)\n",
    "    save_checkpoint({\n",
    "        \"epoch\": epoch + 1 + epoch_checkpoint,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best_valid_loss\": best_valid_loss\n",
    "    }, is_best, os.path.join(experiment_path, \"model.pkl\"))\n",
    "\n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(os.path.join(experiment_path, \"all_scalars.json\"))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval(context, question):\n",
    "    with open(os.path.join(data_dir, \"train\", \"word2idx.pkl\"), \"rb\") as wi, \\\n",
    "         open(os.path.join(data_dir, \"train\", \"char2idx.pkl\"), \"rb\") as ci, \\\n",
    "         open(os.path.join(data_dir, \"train\", \"word_embeddings.pkl\"), \"rb\") as wb, \\\n",
    "         open(os.path.join(data_dir, \"train\", \"char_embeddings.pkl\"), \"rb\") as cb:\n",
    "        word2idx = pickle.load(wi)\n",
    "        char2idx = pickle.load(ci)\n",
    "        word_embedding_matrix = pickle.load(wb)\n",
    "        char_embedding_matrix = pickle.load(cb)\n",
    "\n",
    "    # transform them into Tensors\n",
    "    word_embedding_matrix = torch.from_numpy(np.array(word_embedding_matrix)).type(torch.float32)\n",
    "    char_embedding_matrix = torch.from_numpy(np.array(char_embedding_matrix)).type(torch.float32)\n",
    "    idx2word = dict([(y, x) for x, y in word2idx.items()])\n",
    "\n",
    "    context = clean_text(context)\n",
    "    context = [w for w in word_tokenize(context) if w]\n",
    "\n",
    "    question = clean_text(question)\n",
    "    question = [w for w in word_tokenize(question) if w]\n",
    "\n",
    "    if len(context) > max_len_context:\n",
    "        print(\"The context is too long. Maximum accepted length is\", max_len_context, \"words.\")\n",
    "    if max([len(w) for w in context]) > max_len_word:\n",
    "        print(\"Some words in the context are longer than\", max_len_word, \"characters.\")\n",
    "    if len(question) > max_len_question:\n",
    "        print(\"The question is too long. Maximum accepted length is\", max_len_question, \"words.\")\n",
    "    if max([len(w) for w in question]) > max_len_word:\n",
    "        print(\"Some words in the question are longer than\", .max_len_word, \"characters.\")\n",
    "    if len(question) < 3:\n",
    "        print(\"The question is too short. It needs to be at least a three words question.\")\n",
    "\n",
    "    context_idx = np.zeros([max_len_context], dtype=np.int32)\n",
    "    question_idx = np.zeros([max_len_question], dtype=np.int32)\n",
    "    context_char_idx = np.zeros([max_len_context, max_len_word], dtype=np.int32)\n",
    "    question_char_idx = np.zeros([max_len_question, max_len_word], dtype=np.int32)\n",
    "\n",
    "    # replace 0 values with word and char IDs\n",
    "    for j, word in enumerate(context):\n",
    "        if word in word2idx:\n",
    "            context_idx[j] = word2idx[word]\n",
    "        else:\n",
    "            context_idx[j] = 1\n",
    "        for k, char in enumerate(word):\n",
    "            if char in char2idx:\n",
    "                context_char_idx[j, k] = char2idx[char]\n",
    "            else:\n",
    "                context_char_idx[j, k] = 1\n",
    "\n",
    "    for j, word in enumerate(question):\n",
    "        if word in word2idx:\n",
    "            question_idx[j] = word2idx[word]\n",
    "        else:\n",
    "            question_idx[j] = 1\n",
    "        for k, char in enumerate(word):\n",
    "            if char in char2idx:\n",
    "                question_char_idx[j, k] = char2idx[char]\n",
    "            else:\n",
    "                question_char_idx[j, k] = 1\n",
    "\n",
    "    model = BiDAF(word_vectors=word_embedding_matrix,\n",
    "                  char_vectors=char_embedding_matrix,\n",
    "                  hidden_size,\n",
    "                  drop_prob)\n",
    "    try:\n",
    "        if cuda:\n",
    "            model.load_state_dict(torch.load(os.path.join(squad_models, \"model_final.pkl\"))[\"state_dict\"])\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(os.path.join(squad_models, \"model_final.pkl\"),\n",
    "                                             map_location=lambda storage, loc: storage)[\"state_dict\"])\n",
    "        print(\"Model weights successfully loaded.\")\n",
    "    except:\n",
    "        pass\n",
    "        print(\"Model weights not found, initialized model with random weights.\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context_idx, context_char_idx, question_idx, question_char_idx = torch.tensor(context_idx, dtype=torch.int64).unsqueeze(0).to(device),\\\n",
    "                                                                         torch.tensor(context_char_idx, dtype=torch.int64).unsqueeze(0).to(device),\\\n",
    "                                                                         torch.tensor(question_idx, dtype=torch.int64).unsqueeze(0).to(device),\\\n",
    "                                                                         torch.tensor(question_char_idx, dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "        pred1, pred2 = model(context_idx, context_char_idx, question_idx, question_char_idx)\n",
    "        starts, ends = discretize(pred1.exp(), pred2.exp(), 15, False)\n",
    "        prediction = \" \".join(context[starts.item(): ends.item() + 1])\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    context = \"Rafael Nadal was born in Manacor, a town on the island of Mallorca in the Balearic Islands,\" \\\n",
    "              \" Spain to parents Ana María Parera and Sebastián Nadal. His father is a businessman, owner of an\" \\\n",
    "              \" insurance company, glass and window company Vidres Mallorca, and a restaurant, Sa Punta. Rafael has a\" \\\n",
    "              \" younger sister, María Isabel. His uncle, Miguel Ángel Nadal, is a retired professional footballer,\" \\\n",
    "              \" who played for RCD Mallorca, FC Barcelona, and the Spanish national team. He idolized Barcelona striker\" \\\n",
    "              \" Ronaldo as a child, and via his uncle got access to the Barcelona dressing room to have a photo with\" \\\n",
    "              \" the Brazilian. Nadal supports football clubs Real Madrid and RCD Mallorca. Recognizing in Rafael a\" \\\n",
    "              \" natural talent, another uncle, Toni Nadal, a former professional tennis player, introduced him to\" \\\n",
    "              \" tennis when he was three years old.\"\n",
    "\n",
    "    questions = [\"Where was born Rafael Nadal?\", \"Who is Rafael Nadal's sister?\",\n",
    "                 \"Who introduced Rafael Nadal to tennis?\",\n",
    "                 \"When was Rafael Nadal introduced to tennis?\",\n",
    "                 \"What striker was Rafael Nadal's idol?\"]\n",
    "\n",
    "    print(\"C:\", context, \"\\n\")\n",
    "    for q in questions:\n",
    "        print(\"Q:\", q)\n",
    "        answer = eval(context, q)\n",
    "        print(\"A:\", answer, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
